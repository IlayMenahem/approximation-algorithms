% set document class and formatting
\documentclass[12pt]{article}

% import packages
\usepackage[utf8]{inputenc}
\usepackage{amsmath, amssymb, amsthm}
\usepackage{graphicx}
\usepackage{geometry} 
\usepackage{algorithm}
\usepackage{algpseudocode}
\usepackage{enumerate}
\usepackage{hyperref}
\usepackage{fancyhdr}


% set title and authors
\title{Approximation Algorithms - Homework 2}
\author{Ilay Menahem, and Aaron Ross}

% Page Setup (matching guidelines: 2.5cm margins)
\geometry{margin=2.5cm}


% Theorem Environments
\newtheorem{theorem}{Theorem}
\newtheorem{lemma}{Lemma}
\newtheorem{claim}{Claim}

% Custom Commands
\newcommand{\eps}{\epsilon}
\newcommand{\R}{\mathbb{R}}
\newcommand{\E}{\mathbb{E}}

\begin{document}

\maketitle

\section*{Problem 1: Weighted Max-Cut Local Search}
auxiliary definitions:
\begin{itemize}
    \item $(S, \bar{S})$: a partition of the vertices of $G$ s.t $S = V \setminus \bar{S}$.
    
    \item $W(S, \bar{S})$: the total weight of edges crossing the cut $(S, \bar{S})$. $W(S, \bar{S}) = \sum_{(u,v) \in E, u \in S, v \in \bar{S}} w(u,v)$.
    
    \item $W_k$: the weight of the cut after $k$ iterations.
    
    \item $W_{total}$: the total weight of all edges in the graph. $W_{total} = \sum_{(u,v) \in E} w(u,v)$.
\end{itemize}

\subsection*{(a) Termination in Polynomial Time}
\subsubsection*{Problem Statement}
for a given graph $G = (V, E)$ with non-negative edge weights $w: E \to \mathbb{R}_{\ge 0}$ and a parameter $\epsilon \in (0, 1)$, show that the number of local improvement iterations until the Modified $LS_{Max-Cut}$ algorithm reaches an approximate local optimum is polynomial in the input size.

\begin{proof}
In the Modified $LS_{Max-Cut}$ algorithm, an iteration consists of moving a vertex $v$ from one side of the partition to the other only if the weight of the cut increases by a factor of more than $(1 + \frac{\epsilon}{n})$.

\begin{lemma}
    For any $k \ge 0$, \begin{equation} \label{eq:W_k_bound}
        W_k \ge \left(1 + \frac{\epsilon}{n}\right)^k W_0
    \end{equation}
    \begin{proof}
        By the definition of the algorithm, for any iteration $i \ge 0$, the new weight $W_{i+1}$ satisfies:
        \begin{equation} \label{eq:W_i_bound}
            W_{i+1} \ge \left(1 + \frac{\epsilon}{n}\right) W_i
        \end{equation}
        By induction on $k$.
        \begin{itemize}
            \item Base case: $k = 0$, $W_0 = W_0$.
            \item Inductive step: assume $W_k \ge \left(1 + \frac{\epsilon}{n}\right)^k W_0$, then $W_{k+1} \ge \left(1 + \frac{\epsilon}{n}\right) W_k \ge \left(1 + \frac{\epsilon}{n}\right)^{k+1} W_0$.
        \end{itemize}
    \end{proof}
\end{lemma}

The weight of any cut is bounded from above by the total weight of all edges in the graph. the final weight $W_{final}$ satisfies $W_{final} \le W_{total}$.
Combining this with inequality \eqref{eq:W_k_bound}, we have:
\[
    W_{total} \ge W_k \ge \left(1 + \frac{\epsilon}{n}\right)^k W_0
\]
\[
    \ln\left(\frac{W_{total}}{W_0}\right) \ge k \ln\left(1 + \frac{\epsilon}{n}\right)
\]
\[
    k \le \frac{\ln(W_{total}/W_0)}{\ln\left(1 + \frac{\epsilon}{n}\right)}
\]

To bound the denominator, we use the known inequality $\ln(1+x) \ge \frac{x}{1+x}$, Substituting $x = \frac{\epsilon}{n}$:
\[
    \ln\left(1 + \frac{\epsilon}{n}\right) \ge \frac{\frac{\epsilon}{n}}{1 + \frac{\epsilon}{n}}
\]
Substituting this back into inequality:
\[
    k \le \frac{\ln(W_{total}/W_0)}{\ln\left(1 + \frac{\epsilon}{n}\right)} \le \frac{\ln(W_{total}/W_0)}{\frac{\frac{\epsilon}{n}}{1 + \frac{\epsilon}{n}}} = \left(1 + \frac{\epsilon}{n}\right) \frac{n}{\epsilon} \ln\left(\frac{W_{total}}{W_0}\right)
\]
Since $\epsilon \in (0, 1)$, we have $(1 + \frac{\epsilon}{n}) < 2$ for $n \ge 1$. Thus:
\[
    k < 2 \frac{n}{\epsilon} \ln\left(\frac{W_{total}}{W_0}\right)
\]

\subsubsection*{Complexity Analysis}
To prove $k$ is polynomial in the input size, we analyze the term $\ln(W_{total})$.
Let $\langle \text{input} \rangle$ denote the size of the input in bits.
\begin{itemize}
    \item The term $\frac{n}{\epsilon}$ is clearly polynomial in the input parameters $n$ and $\frac{1}{\epsilon}$.
    \item Let $w_{max}$ be the maximum edge weight. The value $W_{total}$ is at most $|E| \cdot w_{max} \le n^2 w_{max}$.
    \item The number of bits required to represent $w_{max}$ is part of the input size. Thus, $\ln(w_{max})$ is proportional to the number of bits and is linear in the input size.
    \item Therefore, $\ln(W_{total}) \le \ln(n^2) + \ln(w_{max}) = 2\ln(n) + \ln(w_{max})$, which is polynomial in the input size.
\end{itemize}

\textbf{Conclusion:}
Since both $\frac{n}{\epsilon}$ and $\ln(W_{total}/W_0)$ are polynomial in the input size (specifically, polynomial in $n$, $1/\epsilon$, and the number of bits representing the weights), the total number of iterations $k$ is polynomial.
\end{proof}

\newpage
\subsection*{(b) Strongly Polynomial Time Analysis}

\textbf{Problem Statement:}
Suppose we change Step 1 of the algorithm to select the initial cut $(S, \bar{S})$ where $S = \{v^*\}$ and $v^* = \arg \max_{v \in V} w(E(\{v\}, V \setminus \{v\}))$. Show that the number of local improvement iterations is strongly polynomial (dependent only on $n$ and $\epsilon$).

\begin{proof}
Let $d(v) = w(E(\{v\}, V \setminus \{v\}))$ denote the weighted degree of vertex $v$.

The new initialization step selects the vertex $v^*$ with the maximum weighted degree. Thus, the weight of the initial cut is $W_0 = d(v^*)$.

\vspace{1em}
\noindent\textbf{Step 1: Lower bound on the initial cut $W_0$}
We know that the sum of the weighted degrees of all vertices equals twice the total edge weight (by the Handshaking Lemma generalized for weighted graphs):
\[
    \sum_{v \in V} d(v) = 2 W_{total}
\]
Since $v^*$ is the vertex with the maximum weighted degree, $d(v^*)$ must be at least the average weighted degree:
\[
    d(v^*) \ge \frac{1}{n} \sum_{v \in V} d(v) = \frac{2 W_{total}}{n}
\]
Thus, we have the lower bound:
\begin{equation} \label{eq:w0_bound}
    W_0 \ge \frac{2 W_{total}}{n}
\end{equation}

\vspace{1em}
\noindent\textbf{Step 2: Upper bound on the ratio $W_{total}/W_0$}
Using the inequality \eqref{eq:w0_bound}, we can bound the ratio of the maximum possible weight to the initial weight:
\[
    \frac{W_{total}}{W_0} \le \frac{W_{total}}{\frac{2 W_{total}}{n}} = \frac{n}{2}
\]

\vspace{1em}
\noindent\textbf{Step 3: Bounding the number of iterations}
From part (a), we established that the number of iterations $k$ is bounded by:
\[
    k < 2 \frac{n}{\epsilon} \ln\left(\frac{W_{total}}{W_0}\right)
\]
Substituting the bound from Step 2 into this inequality:
\[
    k < 2 \frac{n}{\epsilon} \ln\left(\frac{n}{2}\right)
\]

\vspace{1em}
\noindent\textbf{Conclusion:}
The bound on the number of iterations $k$ is now $O(\frac{n}{\epsilon} \ln n)$
\end{proof}

\newpage

\section*{Problem 2: Hitting Set}
\textbf{Input:} A collection of non-empty sets $\mathcal{C} = \{S_1, \dots, S_m\}$ over a universe $U$, and a non-negative weight function $w: U \to \mathbb{R}_{\ge 0}$.\\
\textbf{Output:} A subset $H \subseteq U$ such that $H \cap S_i \neq \emptyset$ for all $i$, minimizing $\sum_{u \in H} w(u)$.\\
\textbf{Parameter:} Let $S_{max} = \max_{S \in \mathcal{C}} |S|$.

\subsection*{The Algorithm}

\fbox{
    \parbox{\textwidth}{
        \textbf{Algorithm: LocalRatioHittingSet($\mathcal{C}, U, w$)}
        \begin{enumerate}
            \item Let $U_0 = \{u \in U \mid w(u) = 0\}$.
            \item \textbf{Check:} If $U_0$ hits all sets in $\mathcal{C}$ (i.e., $\forall S \in \mathcal{C}, S \cap U_0 \neq \emptyset$):
            \begin{itemize}
                \item Return $U_0$.
            \end{itemize}
            \item \textbf{Select:} Pick any set $S \in \mathcal{C}$ such that $S \cap U_0 = \emptyset$.
            \item \textbf{Define $\epsilon$:} Let $\epsilon = \min_{u \in S} w(u)$.
            \item \textbf{Decompose Weight:} Define $w = w_1 + w_2$ as follows:
                \[ w_1(u) = \begin{cases} \epsilon & \text{if } u \in S \\ 0 & \text{otherwise} \end{cases} \]
                \[ w_2(u) = w(u) - w_1(u) \]
            \item \textbf{Recurse:} Let $H = \text{LocalRatioHittingSet}(\mathcal{C}, U, w_2)$.
            \item \textbf{Return:} $H$.
        \end{enumerate}
    }
}

\subsection*{Proof of Validity}

!!! TODO: Prove the algorithm returns a valid hitting set. !!!

\subsection*{Approximation Analysis}

\begin{theorem}
The algorithm produces an $S_{max}$-approximation for the Weighted Hitting Set problem.
\end{theorem}

\begin{proof}
We proceed by induction on the size of the set $U^+ = \{u \in U \mid w(u) > 0\}$, which is the number of elements with strictly positive weight.

\paragraph{Base Case:}
If the algorithm terminates at Step 2, it returns $U_0$. For all $u \in U_0$, $w(u) = 0$. Thus, the total cost is $w(U_0) = 0$. Since the optimal cost is non-negative, $w(U_0) \le S_{max} \cdot OPT$, satisfying the approximation ratio.

\paragraph{Inductive Step:}
Let $k = |U^+|$ be the number of elements with strictly positive weight in $w$. Assume the approximation holds for any instance where the number of positive-weight elements is strictly less than $k$.

Consider the weight function $w_2$ defined in Step 5. By our choice of $\epsilon = \min_{u \in S} w(u)$, there is at least one element $u^* \in S$ such that $w(u^*) = \epsilon$. Consequently, for this element, $w_2(u^*) = w(u^*) - \epsilon = 0$.
Since $w_2(u) \le w(u)$ for all $u$, and at least one element ($u^*$) has transitioned from positive weight in $w$ to zero weight in $w_2$, the number of positive-weight elements in $w_2$ is strictly less than $k$.

Therefore, we can apply the inductive hypothesis to the recursive call on $(\mathcal{C}, U, w_2)$. This call returns a set $H$ satisfying:
\begin{equation} \label{eq:induct}
    w_2(H) \le S_{max} \cdot OPT(w_2)
\end{equation}
where $OPT(w_2)$ is the weight of the optimal hitting set under $w_2$.

We analyze the approximation with respect to the "easy" weight function $w_1$:
\begin{enumerate}
    \item \textbf{Cost of H under $w_1$:}
    Since $w_1(u) = \epsilon$ for $u \in S$ and 0 otherwise:
    \[ w_1(H) = \sum_{u \in H} w_1(u) = \sum_{u \in H \cap S} \epsilon = |H \cap S| \cdot \epsilon \]
    Since $H$ is a valid hitting set, it must hit $S$. Regardless of which elements it picks, we know $|H \cap S| \le |S|$. Furthermore, by definition $S_{max} \ge |S|$. Thus:
    \[ w_1(H) \le |S| \cdot \epsilon \le S_{max} \cdot \epsilon \]
    
    \item \textbf{Cost of OPT under $w_1$:}
    Any feasible hitting set $H^*$ must hit the specific set $S$ selected in Step 3. Therefore, $H^* \cap S \neq \emptyset$. The minimum cost to hit $S$ under $w_1$ is exactly $\epsilon$ (by picking any single element $u \in S$). Thus:
    \[ OPT(w_1) \ge \epsilon \]
    
    \item \textbf{Ratio for $w_1$:} Combining the above:
    \[ w_1(H) \le S_{max} \cdot \epsilon \le S_{max} \cdot OPT(w_1) \]
\end{enumerate}

\paragraph{Total Weight:}
The total weight of the solution $H$ under $w$ is:
\[ w(H) = w_1(H) + w_2(H) \]
Using the inductive hypothesis (\ref{eq:induct}) and the bound for $w_1$:
\[ w(H) \le S_{max} \cdot OPT(w_1) + S_{max} \cdot OPT(w_2) \]
\[ w(H) \le S_{max} \cdot (OPT(w_1) + OPT(w_2)) \]

Since any optimal solution $H^*$ for $w$ is also a valid hitting set for $w_1$ and $w_2$, we have $OPT(w_1) + OPT(w_2) \le w_1(H^*) + w_2(H^*) = w(H^*) = OPT(w)$. Therefore:
\[ w(H) \le S_{max} \cdot OPT(w) \]
\end{proof}

\subsection*{Complexity Analysis}

\begin{itemize}
    \item \textbf{Recursive Depth:} In every recursive step, we select a set $S$ that is \textit{not} currently hit by $U_0$. We calculate $\epsilon = \min_{u \in S} w(u)$. After subtracting $w_1$, at least one element $u^* \in S$ (specifically the one with weight $\epsilon$) will have its weight reduced to 0 in $w_2$.
    \item Consequently, the number of elements with positive weight strictly decreases in each recursive call. Therefore, there are at most $|U| = n$ recursive calls.
    \item \textbf{Per-Step Cost:} Finding a set unhit by $U_0$ takes $O(m \cdot S_{max})$. Updating weights takes $O(S_{max})$ because we only update weights for one set.
    \item \textbf{Total Complexity:} The algorithm runs in polynomial time, specifically $O(n \cdot m \cdot S_{max})$.
\end{itemize}

\newpage

\section*{Problem 3: Multiple Choice Maximum Coverage}
\subsection*{Problem Statement}
Let the universe of elements be $\Omega$. We are given $k$ collections $\mathcal{C}_1, \dots, \mathcal{C}_k$.
Let $J = (j_1, \dots, j_k)$ be the indices of the sets chosen by the \textbf{local search} algorithm.
Let $S_{local} = \bigcup_{i=1}^k S_{i, j_i}$ be the set of elements covered by the local solution.
Let $O = (o_1, \dots, o_k)$ be the indices of the sets chosen by the \textbf{optimal} solution.
Let $S_{opt} = \bigcup_{i=1}^k S_{i, o_i}$ be the set of elements covered by the optimal solution.

\subsection*{Definitions from Hint}
For each $i \in \{1, \dots, k\}$, let $U_{-i}$ denote the union of all sets chosen by the local solution \textit{except} the one from collection $i$:
\[ U_{-i} = \bigcup_{r \in \{1, \dots, k\} \setminus \{i\}} S_{r, j_r} \]

Using this notation, we define the sets $A_i$ and $B_i$ as given in the hint:
\begin{align*}
    A_i &= S_{i, j_i} \setminus U_{-i} \\
    B_i &= S_{i, o_i} \setminus U_{-i}
\end{align*}

\begin{itemize}
    \item $A_i$ represents the \textbf{unique contribution} of the local set $S_{i, j_i}$ to the current cover $S_{local}$.
    \item $B_i$ represents the \textbf{potential contribution} of the optimal set $S_{i, o_i}$ if we were to swap it into the local solution at index $i$.
\end{itemize}

\subsection*{Proof of Validity}

!!! TODO: Prove the algorithm returns a valid solution. !!!

\subsection*{Approximation Analysis}

\begin{lemma} \label{lemma:local_opt}
For every $i \in \{1, \dots, k\}$, $|B_i| \le |A_i|$.
\end{lemma}

\begin{proof}
Since $J = (j_1, \dots, j_k)$ is returned by the algorithm, it is a local optimum. This means that changing any single index $j_i$ to another index (specifically $o_i$) cannot strictly increase the size of the union.

The size of the current local solution is:
\[ |S_{local}| = |S_{i, j_i} \cup U_{-i}| = |U_{-i}| + |S_{i, j_i} \setminus U_{-i}| = |U_{-i}| + |A_i| \]

Consider the neighbor solution $J'$ where we replace $j_i$ with $o_i$. The set covered by this new solution is $S' = S_{i, o_i} \cup U_{-i}$. The size of this new solution is:
\[ |S'| = |S_{i, o_i} \cup U_{-i}| = |U_{-i}| + |S_{i, o_i} \setminus U_{-i}| = |U_{-i}| + |B_i| \]

By the local optimality condition, $|S'| \le |S_{local}|$. Therefore:
\[ |U_{-i}| + |B_i| \le |U_{-i}| + |A_i| \implies |B_i| \le |A_i| \]
\end{proof}

\begin{theorem}
The local search algorithm is a $\frac{1}{2}$-approximation. That is, $|S_{local}| \ge \frac{1}{2} |S_{opt}|$.
\end{theorem}

\begin{proof}
We can decompose the optimal solution size into two parts: elements that are already covered by our local solution, and elements that are not.
\[ |S_{opt}| = |S_{opt} \cap S_{local}| + |S_{opt} \setminus S_{local}| \]

\textbf{Bounding the first term:}
Trivially, $|S_{opt} \cap S_{local}| \le |S_{local}|$.

\textbf{Bounding the second term:}
Consider an element $x \in S_{opt} \setminus S_{local}$.
Since $x \in S_{opt}$, there must exist some index $i$ such that $x \in S_{i, o_i}$.
Since $x \notin S_{local}$, it means $x$ is not covered by any set in the local solution. Specifically, $x \notin S_{i, j_i}$ and $x \notin U_{-i}$.

Because $x \in S_{i, o_i}$ and $x \notin U_{-i}$, by definition $x \in B_i$.
Therefore, every element in $S_{opt} \setminus S_{local}$ must belong to at least one set $B_i$:
\[ S_{opt} \setminus S_{local} \subseteq \bigcup_{i=1}^k B_i \]

Taking the cardinality:
\[ |S_{opt} \setminus S_{local}| \le \sum_{i=1}^k |B_i| \]

Using Lemma \ref{lemma:local_opt}, we know $\sum |B_i| \le \sum |A_i|$.
\[ |S_{opt} \setminus S_{local}| \le \sum_{i=1}^k |A_i| \]

Now, observe the sets $A_i$. By definition, $A_i$ consists of elements in $S_{i, j_i}$ that are \textit{not} in any other $S_{r, j_r}$. Therefore, the sets $A_1, \dots, A_k$ are pairwise disjoint subsets of $S_{local}$.
\[ \sum_{i=1}^k |A_i| = \left| \bigcup_{i=1}^k A_i \right| \le |S_{local}| \]

\textbf{Combining the bounds:}
\begin{align*}
    |S_{opt}| &= |S_{opt} \cap S_{local}| + |S_{opt} \setminus S_{local}| \\
    &\le |S_{local}| + \sum_{i=1}^k |A_i| \\
    &\le |S_{local}| + |S_{local}| \\
    &= 2 |S_{local}|
\end{align*}

Thus, $|S_{local}| \ge \frac{1}{2} |S_{opt}|$.
\end{proof}

\subsection*{Complexity Analysis}

Let $n = |\Omega|$ be the total number of elements in the universe.
Let $M = \sum_{i=1}^k \ell_i$ be the total number of sets available across all $k$ collections.

\begin{itemize}
    \item \textbf{Monotonic Improvement:} In each iteration of the local search, the algorithm only updates the current solution if the total value $V(j_1, \dots, j_k)$ strictly increases.
    \item \textbf{Bounded Iterations:} Since the value corresponds to the number of covered elements, it is an integer bounded between $0$ and $n$. Therefore, the value can increase at most $n$ times. This implies the `while` loop runs at most $n$ times.
    \item \textbf{Cost per Iteration:} In each iteration, the algorithm checks every possible single-swap neighbor. There are exactly $M - k$ such neighbors. Calculating the coverage of a neighbor takes polynomial time in the input size.
    \item \textbf{Conclusion:} The total running time is bounded by $O(n \cdot M \cdot \text{poly}(\text{input size}))$, which is polynomial.
\end{itemize}

\newpage

\section*{Problem 4: Generalized Steiner Forest (GSF)}
Let $G = (V, E)$ be a connected undirected graph and $T_1, T_2, \dots, T_t \subseteq V$. Let $w : E \to \R_{\ge 0}$ be a non-negative weight function over the edges. A generalized Steiner forest is a subset $E' \subseteq E$, such that for every $u, v \in T_i$ ($1 \le i \le t$) there is a path in $E'$ connecting $u$ and $v$. In the generalized Steiner forest problem the objective is to find a generalized Steiner forest of minimum weight.
(a) Let $T = \bigcup_{i=1}^t T_i$. Assume $|T_i| \ge 2$ for every $1 \le i \le t$ and consider $w' : E \to \R$ defined as follows. For an edge $(u, v)$, $w'(u, v) = |\{u, v\} \cap T|$. Show that any minimal generalized Steiner forest of $G$ is a 2-approximation with respect to the weight function $w'$. Note: a generalized Steiner forest $E'$ of $G$ is minimal if there is no $F \subsetneq E'$ which is also a generalized Steiner forest of $G$.
(b) Suggest a 2-approximation algorithm for the generalized Steiner forest problem using the local ratio technique.

\subsection*{(a) Minimal GSF is a 2-approximation for $w'$}
\textbf{Claim:} Any minimal generalized Steiner forest $E'$ of $G$ is a 2-approximation with respect to the weight function $w'$.

\begin{lemma}
    For any $T$ and $E$ the following equality holds: \[ w'(E) = \sum_{v \in T} \deg_{E}(v) \]
    
    \begin{proof}
        \[ w'(E) = \sum_{(u,v) \in E} w'(u,v) = \sum_{(u,v) \in E} |\{u,v\} \cap T| = \sum_{(u,v) \in E} \left( \mathbf{1}_{u \in T} + \mathbf{1}_{v \in T} \right) \] \[ = \sum_{v \in T} |\{(u,v) \in E\}| = \sum_{v \in T} \deg_{E}(v) \]
    \end{proof}
\end{lemma}


\begin{proof}
    \textbf{Lower Bound on $OPT$:} Consider a optimal solution $E^*$. Since $\forall 1 \le i \le t, |T_i| \ge 2$, we know that any $v \in T$ must be connected to at least one other vertex in $T_i$ via some edge in $E^*$. Thus, each vertex in $T$ contributes at least 1 to the total weight of $E^*$. Therefore, we have:
    \[ OPT = w'(E^*) = \sum_{v \in T} \deg_{E^*}(v) \ge \sum_{v \in T} 1 = |T| \]

    \textbf{Upper Bound on $E'$:} Let $E'$ be a minimal generalized Steiner forest of $G$. Let $1 \le k \le t$ be the number of connected components in the subgraph $G'= (V, E')$, Since $E'$ is minimal, removing any edge from any component would disconnect a path between some $u, v \in T_i$, Thus, each component is a tree. a connected subgraph of a tree is a tree, we will mark every connected component of vertices of $T$ in the graph $G'$ as $C_1, C_2, \dots, C_k$. For a tree with $m$ vertices, the sum of degrees is $2(m-1)$. 
    \[ w'(E') = \sum_{u \in T} \deg_{E'}(u) = \sum_{C_i} 2(|C_i| - 1) \le 2(|T| - 1) - k \le 2|T|\]

    \textbf{Ratio:} Combining the bounds, we have:
    \[ w'(E') \le 2|T| \le 2 OPT \]
    Thus, $E'$ is a 2-approximation with respect to the weight function $w'$.
\end{proof}

\subsection*{(b) Local Ratio Algorithm}
\subsubsection*{Algorithm}
here are three primitive procedures that are used in the algorithm:

\begin{itemize}
    \item $\textsc{isGsf}(F, \{T_1, \dots, T_k\})$: This procedure checks if the forest $F$ is a generalized Steiner forest.
    \begin{algorithm}[H]
    \caption{isGsf($F, \{T_1, \dots, T_k\}$)}
    \begin{algorithmic}[1]
        \State $G_F \gets (V,F)$
        \For{$i \in \{1, \dots, k\}$}
            \If{not \textsc{isConnected}($G_F, T_i$)}
                \State \Return false
            \EndIf
        \EndFor
        \State \Return true
    \end{algorithmic}
    \end{algorithm}
    The complexity of this algorithm is $O(k \cdot (|V| + |F|))$.
    
    \item $\textsc{UpdateActiveSet}(e=(u,v),T)$: This procedure propagates the "active" status to new nodes (the "infection" step).
    \begin{itemize}
        \item If $u \in T$ and $v \notin T$, update $T \leftarrow T \cup \{v\}$.
        \item If $v \in T$ and $u \notin T$, update $T \leftarrow T \cup \{u\}$.
        \item If both or neither are in $T$, do nothing.
    \end{itemize}
    The complexity of this algorithm is $O(\log(|T|))$.

    \item \textsc{PruneForest}$(F, S, \{T_1, T_2, \dots, T_t\})$: This procedure makes the forest $F$ minimal by removing any edge whose deletion does not break the generalized Steiner property.
    \begin{algorithm}[H]
    \caption{PruneForest($F, S, \{T_1, T_2, \dots, T_t\}$)}
    \begin{algorithmic}[1]
        \While{$S \neq \emptyset$}
            \State $e \leftarrow \textsc{Pop}(S)$
            \If{\textsc{isGsf}($F \setminus \{e\}$, $\{T_1, \dots, T_t\}$)}
                \State $F \leftarrow F \setminus \{e\}$
            \EndIf
        \EndWhile
    \end{algorithmic}
    \end{algorithm}
    The complexity of this algorithm is $O(|S| \cdot k (|V| + |F|))$ in the worst case.
\end{itemize}

\begin{algorithm}[H]
\caption{2-Approximation for Generalized Steiner Forest using Local Ratio}
\begin{algorithmic}[1]
    \Function{GSF}{$G = (V, E), \{T_1, T_2, \dots, T_t\}, w$}
        \State $F \leftarrow \emptyset$
        \State $\mathcal{T} \leftarrow \bigcup_{i=1}^k T_i$
        \State $S \leftarrow Stack(\emptyset)$
        \While{NOT \textsc{isGsf}($F, \{T_i\}$)} 
            \State $E_{active} \leftarrow \{ (u,v) \in E \mid \{u,v\} \cap \mathcal{T} \neq \emptyset \}$
            \State $w'(e) \gets \begin{cases} |\{u,v\} \cap \mathcal{T}| & \text{if } e = (u,v) \in E_{active} \\ 0 & \text{otherwise} \end{cases}$
            \State $\epsilon \gets \min_{e \in E_{active}} \frac{w(e)}{w'(e)}$ 
            \State $w(e) \leftarrow w(e) - \epsilon \cdot w'(e)$ 
            \State $E^* \gets \{e \in E_{active} \mid w(e) = 0\} \setminus F$
            \State $F \leftarrow F \cup E^*$ 
            \State Push each $e \in E^*$ onto $S$ 
            \State for each $e \in E^*$ do: \textsc{UpdateActiveSet}($e, \mathcal{T}$)
        \EndWhile
        \State \textsc{PruneForest}$(F, S, \{T_1, T_2, \dots, T_t\})$
        \State \Return $F$
    \EndFunction
    \end{algorithmic}
\end{algorithm}

\subsubsection*{Proof of Correctness}

\subsubsection*{Approximation Analysis}

\subsubsection*{Complexity Analysis}

\newpage

\section*{Problem 5: Knapsack with Partition Constraints}

Consider the following variant of the Knapsack problem. The input consists of $n$ items $I = \{1, \ldots, n\}$, where item $i \in I$ has a weight $w_i \in \mathbb{Z}^+$ and a value $v_i \in \mathbb{Z}^+$. The items are partitioned into disjoint sets $S_1, S_2, \ldots, S_m$; that is, $\bigcup_{j \in [m]} S_j = I$ and $S_j \cap S_\ell = \emptyset$ for all $j \neq \ell \in [m]$. Moreover, each set $S_j$ has a bound $k(j)$. Also, we are given a knapsack capacity $B \in \mathbb{Z}^+$. A solution for the problem is a subset of items $P \subseteq I$ such that $\sum_{i \in P} w_i \leq B$ and $|P \cap S_j| \leq k(j)$ for all $j \in [m]$. The value of the solution $P$ is $\sum_{i \in P} v_i$. The objective is to find a solution $P$ of maximal value. Obtain an FPTAS for the problem.

\subsection*{Overview}
we will maintain a \emph{domination list} (a list of nondominated $(\text{weight},\text{value})$ pairs).
A pair $(w,v)$ \emph{dominates} $(w',v')$ if $w \le w'$ and $v \ge v'$.
Dominated pairs can be discarded without affecting optimality.

For the FPTAS, we will apply the standard scaling technique (as in the usual knapsack FPTAS) and run the same domination-list DP on the scaled instance.

\subsection*{Algorithm Description}
\paragraph{Domination-list primitives.}
\begin{itemize}
    \item $\textsc{Shift}(L, \Delta w, \Delta v) = \{(w+\Delta w, v+\Delta v) : (w,v)\in L\}$.
    \item $\textsc{Prune}(L)$ removes all dominated pairs from $L$ (and may also remove pairs with $w>B$). One implementation: sort pairs by increasing weight; sweep, keeping a pair only if its value is strictly larger than the maximum value seen so far. Its complexity is $O(|L| \log |L|)$ due to sorting.
    \item $\textsc{Convolve}(L_1, L_2)$ computes all pairwise sums of pairs from $L_1$ and $L_2$: \[ \textsc{Convolve}(L_1, L_2) = \{(w_1 + w_2, v_1 + v_2) : (w_1, v_1) \in L_1, (w_2, v_2) \in L_2\} \]. Its complexity is $O(|L_1| \cdot |L_2|)$.
\end{itemize}

\begin{algorithm}[H]
\caption{Knapsack with Partition Constraints (Domination Lists)}
\begin{algorithmic}[1]
\Function{Knapsack-Partition}{$I, S_1, S_2, \ldots, S_m, k, B$}
    \State $L \gets \{(0,0)\}$ \Comment{global nondominated set of feasible (weight,value) pairs}
    \For{each partition $S_j$}
        \State \Comment{Compute nondominated options using items from $S_j$ with at most $k(j)$ picks}
        \State $L^{(0)} \gets \{(0,0)\}$
        \For{$c=1$ to $k(j)$} $L^{(c)} \gets \emptyset$ \EndFor
        \For{each item $i \in S_j$}
            \For{$c$ from $k(j)$ down to $1$}
                \State $L^{(c)} \gets \textsc{Prune}\Big( L^{(c)} \cup \textsc{Shift}(L^{(c-1)}, w_i, v_i)\Big)$
            \EndFor
        \EndFor
        \State \Comment{Combine the chosen items from $S_j$ with the global list}
        \State $L_{\text{next}} \gets \emptyset$
        \For{$c=0$ to $k(j)$}
            \State $L_{\text{next}} \gets \textsc{Prune}(L_{\text{next}} \cup \textsc{Convolve}(L, L^{(c)}))$
        \EndFor
        \State $L \gets L_{\text{next}}$
    \EndFor
    \State \Return $\max\{v : (w,v)\in L\}$
\EndFunction
\end{algorithmic}
\end{algorithm}


\subsection*{Proof of Correctness}
Without loss of generality, assume $\forall j,\ k(j) \le |S_j|$ (otherwise replace $k(j)$ by $|S_j|$).

\begin{lemma}
    After processing partitions $S_1,\dots,S_j$, the list $L$ contains exactly the nondominated pairs $(w,v)$ such that there exists a feasible selection of items from the first $j$ partitions of total weight $w$ and total value $v$ (respecting all bounds).
\end{lemma}

\begin{proof}
    \textbf{Base case ($j=0$):} $L=\{(0,0)\}$ is the unique feasible pair (select nothing), and it is nondominated.

    \textbf{Inductive step:} Assume the claim holds after $S_1,\dots,S_{j-1}$. The lists $L^{(c)}$ are constructed with an explicit counter $c$, and $\textsc{Prune}$ only removes dominated pairs, hence each $L^{(c)}$ represents exactly the nondominated achievable pairs using exactly $c$ items from $S_j$. 
    When convolving $L^{(c)}$ with $L_{\text{new}}$, we consider all ways to add $c$ items from $S_j$ to previously selected items (from partitions $1$ to $j-1$). The final pruning step ensures that only nondominated pairs remain in $L_{\text{new}}$. Thus, after processing $S_j$, $L$ contains exactly the nondominated pairs corresponding to feasible selections from the first $j$ partitions.
\end{proof}

\begin{theorem}
    The algorithm returns the optimal objective value for the knapsack with partition constraints problem.
\end{theorem}

\begin{proof}
    After processing all partitions, every feasible solution corresponds to some pair in the (unpruned) constructed set, and pruning preserves at least one representative of every attainable optimum value (for some weight). Thus $\max\{v:(w,v)\in L\}$ equals the optimal objective value under capacity $B$ and the partition bounds.
\end{proof}

\subsection*{Time Complexity}
We will note $V = \sum_{i \in I} v_i$, and $k_{\max} = \max_{j \in [m]} k(j) \le n$.
The outer loop runs $m$ times (once per partition).

Inside, we have two main parts:
\begin{itemize}
    \item Updating $L^{(c)}$ for each item in $S_j$: we do $|S_j| \le n$ iterations, each involving at most $k_{\max}$ shifts and prunes. Each prune takes $O(|L^{(c)}| \log |L^{(c)}|)$ time, and $|L^{(c)}| \le \min(B, V)$ (since weights are at most $B$ and values at most $V$). Thus this part takes $O(n \cdot k_{\max} \cdot \min(B, V) \log(\min(B, V)))$ time per partition.
    \item Combining $L^{(c)}$ into $L_{\text{next}}$: we do at most $k_{\max}+1$ convolutions and prunes. Each convolution takes $O(|L| \cdot |L^{(c)}|) \le O(\min(B, V)^2)$ time, and each prune takes $O(|L_{\text{next}}| \log |L_{\text{next}}|) \le O(\min(B, V) \log(\min(B, V)))$ time. Thus this part takes $O(k_{\max} \cdot \min(B, V)^2)$ time per partition.
\end{itemize}

Thus the total complexity is $O(m \cdot n \cdot k_{\max} \cdot \min(B, V)^2) = O(m \cdot n^2 \cdot \min(B, V)^2)$, which is polynomial in $n$, $m$, and $\min(B, V)$.

\subsection*{FPTAS Construction}
To enable the FPTAS, we will use the standard scaling technique. Let $V_{\max} = \max_{i \in I} v_i$, $K = \frac{\epsilon V_{\max}}{n}$.

\begin{algorithm}[H]
\caption{FPTAS for Knapsack with Partition Constraints}
    \begin{algorithmic}[1]
    \Function{FPTAS-Knapsack-Partition}{$I, S_1, S_2, \ldots, S_m, k, B$}
        \State Scale values: for each $i \in I$, set $v'_i = \lfloor v_i / K \rfloor$
        \State \Return \Call{Knapsack-Partition}{$I, S_1, S_2, \ldots, S_m, k, B$}
    \EndFunction
    \end{algorithmic}
\end{algorithm}

\begin{proof}
    \textbf{Time Complexity:} The time complexity is $O\left(m \cdot n^2 \cdot \min(B, V')^2\right)$
    \[\min(B, V') \le V' = \sum_{i \in I} v'_i \le \sum_{i \in I} \frac{v_i}{K} = \frac{\sum_{i \in I} v_i}{K} \le \frac{n V_{\max}}{K} = \frac{n V_{\max}}{\epsilon V_{\max} / n} = \frac{n^2}{\epsilon}\]
    Thus the time complexity is $O\left(m \cdot n^2 \cdot \left(\frac{n^2}{\epsilon}\right)^2\right) = O\left(\frac{m \cdot n^6}{\epsilon^2}\right)$, which is polynomial in $n$, $m$, and $\frac{1}{\epsilon}$.
\end{proof}

\begin{proof}
    \textbf{Approximation Ratio:} For the optimal set $P^*$, the difference between the true value and the scaled value is:
    \[ \sum_{i \in P^*} v_i - K \cdot \sum_{i \in P^*} v'_i < \sum_{i \in P^*} K = |P^*| K \le n K = \epsilon V_{\max} \le \epsilon \cdot OPT \]
    Thus, the solution returned is at least $(1-\epsilon) \cdot OPT$.
\end{proof}

\end{document}